{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7977297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tenserflow (from versions: none)\n",
      "ERROR: No matching distribution found for tenserflow\n"
     ]
    }
   ],
   "source": [
    "! pip install tenserflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be010678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9efa824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ca548a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db93be11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VICTUS\\Music\\anaconda\\envs\\tf_env\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn.add(Conv2D(32,(3,3),input_shape=(64,64,3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn.add(Conv2D(16,(3,3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60796f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(64,activation='relu'))\n",
    "cnn.add(Dense(32,activation='relu'))\n",
    "cnn.add(Dense(16,activation='relu'))\n",
    "cnn.add(Dense(8,activation='relu'))\n",
    "cnn.add(Dense(4,activation='relu'))\n",
    "cnn.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8812d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64982066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "{'cats_set': 0, 'dogs_set': 1}\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 0.6929 - val_loss: 0.6900\n",
      "Epoch 2/5\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 0.6891 - val_loss: 0.6812\n",
      "Epoch 3/5\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.6817 - val_loss: 0.6910\n",
      "Epoch 4/5\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.6749 - val_loss: 0.6519\n",
      "Epoch 5/5\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.6613 - val_loss: 0.6525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2400e24f880>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\VICTUS\\Downloads\\CNN\\TrainData\",\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\VICTUS\\Downloads\\CNN\\TestData\",\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=5,\n",
    "    validation_data=test_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37e0703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c9c7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.load_img(r\"C:\\Users\\VICTUS\\Downloads\\CNN\\TrainData\\dogs_set\\dog.4477.jpg\", target_size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "690d4c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72fd107c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[195., 205., 241.],\n",
       "        [185., 199., 234.],\n",
       "        [192., 211., 244.],\n",
       "        ...,\n",
       "        [189., 197., 234.],\n",
       "        [170., 175., 205.],\n",
       "        [143., 135., 150.]],\n",
       "\n",
       "       [[198., 207., 248.],\n",
       "        [192., 206., 245.],\n",
       "        [188., 206., 244.],\n",
       "        ...,\n",
       "        [204., 218., 255.],\n",
       "        [184., 193., 234.],\n",
       "        [150., 145., 177.]],\n",
       "\n",
       "       [[190., 205., 244.],\n",
       "        [185., 201., 235.],\n",
       "        [185., 199., 244.],\n",
       "        ...,\n",
       "        [223., 227., 254.],\n",
       "        [204., 208., 253.],\n",
       "        [180., 188., 227.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 98.,  77.,  82.],\n",
       "        [ 98.,  82.,  83.],\n",
       "        [ 94.,  82.,  82.],\n",
       "        ...,\n",
       "        [101.,  84.,  77.],\n",
       "        [ 97.,  78.,  72.],\n",
       "        [ 98.,  74.,  70.]],\n",
       "\n",
       "       [[ 93.,  78.,  75.],\n",
       "        [ 84.,  66.,  64.],\n",
       "        [ 98.,  80.,  78.],\n",
       "        ...,\n",
       "        [ 99.,  84.,  79.],\n",
       "        [ 99.,  80.,  76.],\n",
       "        [107.,  83.,  81.]],\n",
       "\n",
       "       [[ 79.,  64.,  59.],\n",
       "        [ 90.,  72.,  68.],\n",
       "        [ 99.,  80.,  76.],\n",
       "        ...,\n",
       "        [ 98.,  84.,  81.],\n",
       "        [101.,  83.,  81.],\n",
       "        [105.,  84.,  81.]]], shape=(64, 64, 3), dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "655e6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "965a84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=np.expand_dims(img,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "438c089c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n"
     ]
    }
   ],
   "source": [
    "p=cnn.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2e5695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "source": [
    "if p[0][0]<0.5:\n",
    "    print(\"cat\")\n",
    "else:\n",
    "    print(\"Dog\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
